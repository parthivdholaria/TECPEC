{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official evaluation script for the ECAC task at SemEval-2024 Task 3\n",
    "# Source: https://github.com/NUSTM/SemEval-2024_ECAC/blob/main/CodaLab/evaluation/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, copy, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_idx = dict(zip(['neutral','anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise'], range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(json_file):\n",
    "    with open(json_file, 'r') as fi:\n",
    "        json_data = json.load(fi)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def convert_list_to_dict(data_list, main_key=''):\n",
    "    new_dict = {}\n",
    "    for x in data_list:\n",
    "        if 'ID' in main_key:\n",
    "            key = int(x[main_key])\n",
    "        else:\n",
    "            key = x[main_key]\n",
    "        if key not in new_dict:\n",
    "            new_dict[key] = x\n",
    "        else:\n",
    "            sys.exit('Instance repeat!')\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# Participants need to provide the position indexes of the cause span in the prediction file!!!\n",
    "# You can use this function to obtain the position indexes of golden annotations.\n",
    "def get_span_position(span, utterance):\n",
    "    begin_id, end_id = 0, 0\n",
    "    cause_token = span.split()\n",
    "    utterance_token = utterance.split()\n",
    "    for wi in range(len(utterance_token)):\n",
    "        if (wi+len(cause_token))<=len(utterance_token) and utterance_token[wi:wi+len(cause_token)] == cause_token:\n",
    "            begin_id = wi\n",
    "            end_id = wi+len(cause_token)\n",
    "            break\n",
    "    return [begin_id, end_id] # start from 0, [begin_id, end_id)\n",
    "\n",
    "\n",
    "'''\n",
    "Strict Match: emotion_utt and cause_utt are the same, and the cause spans completely match.\n",
    "Fuzzy Match: emotion_utt and cause_utt are the same, and the cause spans overlap\n",
    "'''\n",
    "def judge_cause_span_pair_emocate(pred_span_pair, true_spans_pos_dict, span_mode='fuzzy'): # strict/fuzzy\n",
    "    d_id, emo_id, cau_id, start_cur, end_cur, emo = pred_span_pair\n",
    "    cur_key = 'dia{}_emoutt{}_causeutt{}'.format(d_id, emo_id, cau_id)\n",
    "    if cur_key in true_spans_pos_dict:\n",
    "        if span_mode == 'strict':\n",
    "            if [start_cur, end_cur, emo] in true_spans_pos_dict[cur_key]:\n",
    "                true_spans_pos_dict[cur_key].remove([start_cur, end_cur, emo])\n",
    "                return True\n",
    "        else:\n",
    "            for t_start, t_end, emo_y in true_spans_pos_dict[cur_key]:\n",
    "                if emo == emo_y and not(end_cur<=t_start or start_cur>=t_end):\n",
    "                    true_spans_pos_dict[cur_key].remove([t_start, t_end, emo_y]) \n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def cal_prf_span_pair_emocate(span_pair_dict, pred_pairs, span_mode='strict'): \n",
    "    conf_mat = np.zeros([7,7])\n",
    "    for p in pred_pairs: # [conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category]\n",
    "        if judge_cause_span_pair_emocate(p, span_pair_dict, span_mode=span_mode):\n",
    "            conf_mat[p[5]][p[5]] += 1\n",
    "        else:\n",
    "            conf_mat[0][p[5]] += 1\n",
    "    for k, v in span_pair_dict.items():\n",
    "        for p in v:\n",
    "            conf_mat[p[2]][0] += 1\n",
    "    p = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 0)+(1e-8), [1,7]))\n",
    "    r = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 1)+(1e-8), [7,1]))\n",
    "    f = 2*p*r/(p+r+(1e-8))\n",
    "    weight0 = np.sum(conf_mat, axis = 1)\n",
    "    weight = weight0[1:] / np.sum(weight0[1:])\n",
    "    w_avg_p = np.sum(p[1:] * weight)\n",
    "    w_avg_r = np.sum(r[1:] * weight)\n",
    "    w_avg_f1 = np.sum(f[1:] * weight)\n",
    "    \n",
    "    micro_acc = np.sum(np.diagonal(conf_mat)[1:])\n",
    "    micro_p = micro_acc / (sum(np.sum(conf_mat, axis = 0)[1:])+(1e-8))\n",
    "    micro_r = micro_acc / (sum(np.sum(conf_mat, axis = 1)[1:])+(1e-8))\n",
    "    micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-8)\n",
    "    \n",
    "    return [w_avg_p, w_avg_r, w_avg_f1, micro_p, micro_r, micro_f1]\n",
    "\n",
    "\n",
    "def get_match_scores(pred_span, true_spans):\n",
    "    match_id, match_gold_length, match_length, match_score = 0, 0, 0, 0\n",
    "    p_start, p_end, p_emo = pred_span\n",
    "    for ii, (t_start, t_end, t_emo) in enumerate(true_spans):\n",
    "        if p_emo == t_emo and not (p_end<=t_start or p_start>=t_end):\n",
    "            cur_match_length = min(p_end, t_end) - max(p_start, t_start)\n",
    "            cur_gold_length = t_end - t_start\n",
    "            cur_match_score = cur_match_length / float(cur_gold_length)\n",
    "            if cur_match_score > match_score:\n",
    "                match_id = ii\n",
    "                match_gold_length = cur_gold_length\n",
    "                match_length = cur_match_length\n",
    "                match_score = cur_match_score\n",
    "            if (cur_match_score == match_score) and (cur_match_score > 0):\n",
    "                if cur_match_length > match_length:\n",
    "                    match_id = ii\n",
    "                    match_gold_length = cur_gold_length\n",
    "                    match_length = cur_match_length\n",
    "                    match_score = cur_match_score\n",
    "    return match_id, match_gold_length, match_length, match_score\n",
    "\n",
    "\n",
    "'''\n",
    "Proportional Match (span): Each predicted span is compared with all golden spans, and determine which golden span it matches based on the overlap ratio (match score). Then the Precision, Recall, and F1 are calculated based on the overlapping tokens.\n",
    "'''\n",
    "def cal_prf_span_pair_emocate_proportional(true_span_pair_dict, pred_span_pair_dict): # 'dia{}_emoutt{}_causeutt{}': [[span_start_id, span_end_id, emotion_category], ...]\n",
    "    prf_mat = np.zeros([7,5]) # row: emotion category; col: correct_num, true_num, pred_num, matched_true_span_num, true_span_num\n",
    "    true_span_pair_dict_copy = copy.deepcopy(true_span_pair_dict)\n",
    "    for k, v in pred_span_pair_dict.items():\n",
    "        for pred_span in v:\n",
    "            prf_mat[pred_span[2]][2] += pred_span[1] - pred_span[0]\n",
    "            if k in true_span_pair_dict:\n",
    "                true_spans = true_span_pair_dict[k]\n",
    "                match_id, match_gold_length, match_length, match_score = get_match_scores(pred_span, true_spans)\n",
    "                if match_length > 0:\n",
    "                    prf_mat[pred_span[2]][0] += match_length\n",
    "                    prf_mat[pred_span[2]][1] += match_gold_length # Multiple predicted spans may match the same golden span.\n",
    "                    prf_mat[pred_span[2]][3] += 1\n",
    "                    if true_spans[match_id] in true_span_pair_dict_copy[k]:\n",
    "                        true_span_pair_dict_copy[k].remove(true_spans[match_id])\n",
    "    \n",
    "    for k, v in true_span_pair_dict_copy.items():\n",
    "        for true_span in v:\n",
    "            prf_mat[true_span[2]][1] += true_span[1] - true_span[0]\n",
    "            prf_mat[true_span[2]][3] += 1\n",
    "    for k, v in true_span_pair_dict.items():\n",
    "        for true_span in v:\n",
    "            prf_mat[true_span[2]][4] += 1\n",
    "    \n",
    "    p_scores = prf_mat[1:,0] / (prf_mat[1:,2]+(1e-8))\n",
    "    r_scores = prf_mat[1:,0] / (prf_mat[1:,1]+(1e-8))\n",
    "    f1_scores = 2*p_scores*r_scores/(p_scores+r_scores+(1e-8))\n",
    "    weight = prf_mat[1:,4] / sum(prf_mat[1:,4]) # Calculate the weight based on the actual number of golden spans.\n",
    "    w_avg_p = sum(p_scores*weight)\n",
    "    w_avg_r = sum(r_scores*weight)\n",
    "    w_avg_f1 = sum(f1_scores*weight)\n",
    "\n",
    "    total_correct = sum(prf_mat[1:,0])\n",
    "    micro_p = total_correct / (sum(prf_mat[1:,2])+(1e-8))\n",
    "    micro_r = total_correct / (sum(prf_mat[1:,1])+(1e-8))\n",
    "    micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-8)\n",
    "\n",
    "    return [w_avg_p, w_avg_r, w_avg_f1, micro_p, micro_r, micro_f1]\n",
    "\n",
    "\n",
    "# Remove the punctuation token at the beginning and end of the cause span\n",
    "def clean_span(span):\n",
    "    while 1:\n",
    "        span = span.strip()\n",
    "        if span[0] not in string.punctuation and span[-1] not in string.punctuation:\n",
    "            break\n",
    "        else:\n",
    "            if span[0] in string.punctuation:\n",
    "                span = span[1:]\n",
    "            if span[-1] in string.punctuation:\n",
    "                span = span[:-1]\n",
    "    return span\n",
    "\n",
    "\n",
    "def has_letter(text):\n",
    "    for c in text:\n",
    "        if c.isalpha():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_1_2(pred_data, gold_data):\n",
    "    gold_data_dict = convert_list_to_dict(gold_data, main_key=\"conversation_ID\")\n",
    "    pred_data_dict = convert_list_to_dict(pred_data, main_key=\"conversation_ID\")\n",
    "    \n",
    "    pred_pairs, true_pairs = [], []\n",
    "    conv_context_dict = {}\n",
    "    for id, ins in gold_data_dict.items(): # The public evaluation data may contain some interference data that is not used for evaluation.\n",
    "        if id not in pred_data_dict:\n",
    "            sys.exit('Conversation {} are missing!'.format(id))\n",
    "        else:\n",
    "            pred = pred_data_dict[id]\n",
    "            all_utterances = [x[\"text\"] for x in ins[\"conversation\"]]\n",
    "            conv_context_dict[id] = all_utterances\n",
    "            \n",
    "            def get_new_pair_list(span_pair_list, pred=False):\n",
    "                new_span_pair_list  = []\n",
    "                for x in span_pair_list:\n",
    "                    if not isinstance(x, list):\n",
    "                        sys.exit('emotion-cause_pairs format error!')\n",
    "                    else:\n",
    "                        if len(x) != 2:\n",
    "                            sys.exit('emotion-cause_pairs format error!')\n",
    "                        else:\n",
    "                            emo_id, emotion = x[0].split('_')\n",
    "                            if emotion not in emotion_idx:\n",
    "                                sys.exit('Unknown emotion category!')\n",
    "                            else:\n",
    "                                if 'U' in emo_id:\n",
    "                                    emo_id = emo_id.replace('U','')\n",
    "                                if pred:\n",
    "                                    if has_letter(x[1]):\n",
    "                                        sys.exit('emotion-cause_pairs format error! You should provide the position index range of the cause span, not the text itself.')\n",
    "                                    else:\n",
    "                                        cause_info = x[1].split('_')\n",
    "                                        if len(cause_info) != 3:\n",
    "                                            sys.exit('emotion-cause_pairs format error!')\n",
    "                                        else:\n",
    "                                            cause_id, span_start_id, span_end_id = cause_info\n",
    "                                            if 'U' in cause_id:\n",
    "                                                cause_id = cause_id.replace('U','')\n",
    "                                            span_idx_list = [int(span_start_id), int(span_end_id)]\n",
    "                                else:\n",
    "                                    cause_id, cur_span = x[1].split('_')\n",
    "                                    if 'U' in cause_id:\n",
    "                                        cause_id = cause_id.replace('U','')\n",
    "                                    cur_span = clean_span(cur_span)\n",
    "                                    span_idx_list = get_span_position(cur_span, all_utterances[int(cause_id)-1])\n",
    "                                \n",
    "                                new_pair = [id, int(emo_id), int(cause_id)] + span_idx_list + [emotion_idx[emotion]]\n",
    "                                if new_pair not in new_span_pair_list:\n",
    "                                    new_span_pair_list.append(new_pair)\n",
    "                return new_span_pair_list # [[conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category], ...]\n",
    "            \n",
    "            true_pairs.extend(get_new_pair_list(ins[\"emotion-cause_pairs\"])) # \n",
    "            \n",
    "            if \"emotion-cause_pairs\" not in pred:\n",
    "                sys.exit(\"Cannot find the key 'emotion-cause_pairs'!\")\n",
    "            else:\n",
    "                pred_pairs.extend(get_new_pair_list(pred[\"emotion-cause_pairs\"], pred=True))\n",
    "    \n",
    "    def get_span_pair_dict(pairs):\n",
    "        span_pair_dict = {}\n",
    "        for p in pairs:\n",
    "            cur_key = 'dia{}_emoutt{}_causeutt{}'.format(p[0], p[1], p[2])\n",
    "            if cur_key in span_pair_dict:\n",
    "                span_pair_dict[cur_key].append(p[3:])\n",
    "            else:\n",
    "                span_pair_dict[cur_key] = [p[3:]]\n",
    "        return span_pair_dict\n",
    "    \n",
    "    true_span_pair_dict = get_span_pair_dict(true_pairs)\n",
    "    pred_span_pair_dict = get_span_pair_dict(pred_pairs)\n",
    "    \n",
    "    true_span_pair_dict_copy = copy.deepcopy(true_span_pair_dict)\n",
    "    score_list = cal_prf_span_pair_emocate(true_span_pair_dict, pred_pairs, span_mode='strict')\n",
    "    score_list_2 = cal_prf_span_pair_emocate_proportional(true_span_pair_dict_copy, pred_span_pair_dict)\n",
    "    return score_list, score_list_2\n",
    "\n",
    "\n",
    "def cal_prf_pair_emocate(true_pairs, pred_pairs):\n",
    "    conf_mat = np.zeros([7,7])\n",
    "    for p in pred_pairs:\n",
    "        if p in true_pairs:\n",
    "            conf_mat[p[3]][p[3]] += 1\n",
    "        else:\n",
    "            conf_mat[0][p[3]] += 1\n",
    "    for p in true_pairs:\n",
    "        if p not in pred_pairs:\n",
    "            conf_mat[p[3]][0] += 1\n",
    "    p = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 0)+(1e-8), [1,7]))\n",
    "    r = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 1)+(1e-8), [7,1]))\n",
    "    f = 2*p*r/(p+r+(1e-8))\n",
    "    weight0 = np.sum(conf_mat, axis = 1)\n",
    "    weight = weight0[1:] / np.sum(weight0[1:])\n",
    "    w_avg_p = np.sum(p[1:] * weight)\n",
    "    w_avg_r = np.sum(r[1:] * weight)\n",
    "    w_avg_f1 = np.sum(f[1:] * weight)\n",
    "    \n",
    "    micro_acc = np.sum(np.diagonal(conf_mat)[1:])\n",
    "    micro_p = micro_acc / (sum(np.sum(conf_mat, axis = 0)[1:])+(1e-8))\n",
    "    micro_r = micro_acc / (sum(np.sum(conf_mat, axis = 1)[1:])+(1e-8))\n",
    "    micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-8)\n",
    "    \n",
    "    results = [micro_p, micro_r, micro_f1, w_avg_p, w_avg_r, w_avg_f1]\n",
    "    return results\n",
    "\n",
    "def evaluate_2_2(pred_data, gold_data):\n",
    "    gold_data_dict = convert_list_to_dict(gold_data, main_key=\"conversation_ID\")\n",
    "    pred_data_dict = convert_list_to_dict(pred_data, main_key=\"conversation_ID\")\n",
    "    \n",
    "    pred_pairs, true_pairs = [], []\n",
    "    for id, ins in gold_data_dict.items():\n",
    "        if id not in pred_data_dict:\n",
    "            sys.exit('Conversation {} are missing!'.format(id))\n",
    "        else:\n",
    "            pred = pred_data_dict[id]\n",
    "            \n",
    "            def get_new_pair(pair_scores):\n",
    "                emo_id, emotion = pair_scores[0].split('_')\n",
    "                if emotion not in emotion_idx:\n",
    "                    sys.exit('Unknown emotion category!')\n",
    "                else:\n",
    "                    if 'U' in emo_id:\n",
    "                        emo_id = emo_id.replace('U','')\n",
    "                    return [id, int(emo_id), int(pair_scores[1]), emotion_idx[emotion]]\n",
    "            \n",
    "            for p in ins[\"emotion-cause_pairs\"]:\n",
    "                new_pair = get_new_pair(p)\n",
    "                if new_pair not in true_pairs:\n",
    "                    true_pairs.append(new_pair)\n",
    "\n",
    "            if \"emotion-cause_pairs\" not in pred:\n",
    "                sys.exit(\"Cannot find the key 'emotion-cause_pairs'!\")\n",
    "            else:\n",
    "                for p in pred[\"emotion-cause_pairs\"]:\n",
    "                    new_pair = get_new_pair(p)\n",
    "                    if new_pair not in pred_pairs:\n",
    "                        pred_pairs.append(new_pair)\n",
    "    \n",
    "    all_results_emocate = cal_prf_pair_emocate(true_pairs, pred_pairs)\n",
    "    return all_results_emocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(os.path.join('scores_val_cause_pred_gpt2.txt'), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = json.load(open(\"D:\\\\Github\\\\NLP-Project-24\\\\CEE\\\\Predictions\\\\GPT2\\\\val_cause_pred_gpt2_submit.json\", 'r'))\n",
    "gold_data = get_json_data(\"D:\\\\Github\\\\NLP-Project-24\\\\Dataset\\\\ERC_conversational_level\\\\val_conversation_level.json\")\n",
    "score_list, score_list_1 = evaluate_1_2(pred_data, gold_data)\n",
    "output_file.write(\"weighted_strict_precision:{}\\n\".format(score_list[0]))\n",
    "output_file.write(\"weighted_strict_recall:{}\\n\".format(score_list[1]))\n",
    "output_file.write(\"weighted_strict_f1:{}\\n\".format(score_list[2])) \n",
    "output_file.write(\"weighted_Proportional_precision:{}\\n\".format(score_list_1[0]))\n",
    "output_file.write(\"weighted_Proportional_recall:{}\\n\".format(score_list_1[1]))\n",
    "output_file.write(\"weighted_Proportional_f1:{}\\n\".format(score_list_1[2])) \n",
    "                \n",
    "output_file.write(\"strict_precision:{}\\n\".format(score_list[3]))\n",
    "output_file.write(\"strict_recall:{}\\n\".format(score_list[4]))\n",
    "output_file.write(\"strict_f1:{}\\n\".format(score_list[5])) \n",
    "output_file.write(\"Proportional_precision:{}\\n\".format(score_list_1[3]))\n",
    "output_file.write(\"Proportional_recall:{}\\n\".format(score_list_1[4]))\n",
    "output_file.write(\"Proportional_f1:{}\\n\".format(score_list_1[5])) \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECPEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
